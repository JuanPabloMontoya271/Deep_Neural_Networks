{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, seq_max_len,state_size, vocab_size, num_classes):\n",
    "        #init de los parametros con el constructor de la clase\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.state_size = state_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "    def build_model():\n",
    "        self.x = tf.placeholder(shape= [None, seq_max_len], dtype= tf.int32)\n",
    "        x_one_hot = tf.one_hot(self.x, self.vocab_size)\n",
    "        x_one_hot = tf.cast(x_one_hot, tf.float32)\n",
    "        rnn_input= tf.nn.unstack(x_one_hot, axis = 1)\n",
    "        self.y = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "        self.y_one_hot = tf.one_hot(self.y, self.num_classes, dtype = tf.float32)\n",
    "        self.batch_size = tf.placeholder(tf.int32, [], name = 'batch_size')\n",
    "        \n",
    "        weights= {\n",
    "            \n",
    "            \n",
    "            'layer_0': tf.Variable(tf.random_normal([self.state_size, 256])),\n",
    "            'layer_1': tf.Variable(tf.random_normal([256, self.num_classes]))\n",
    "        }\n",
    "        \n",
    "        biases = {\n",
    "            \n",
    "            \n",
    "            'layer_0': tf.Variable(tf.random_normal([256])),\n",
    "            'layer_1': tf.Variable(tf.random_normal([self.num_classes]))\n",
    "        }\n",
    "        init_state = tf.zeros([self.batch_size, self.state_size])\n",
    "        cell  = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "        self.outputs, self.final_state = tf.contrib.rnn.static_rnn(cell, rnn_input)\n",
    "        output = self.outputs[-1]\n",
    "        hidden = tf.matmul(output, weights['layer_0']) + biases ['layer_0']\n",
    "        hidden = tf.nn.tanh(hidden)\n",
    "        self.logits = tf.matmul(hidden, weights['layer_1']) + biases ['layer_1']\n",
    "        self.probs = tf.nn.softmax(logits)\n",
    "        self.correct_preds = tf.equal(tf.argmax(self.probs,axis= 1),tf.argmax(self.y_one_hot, axis = 1))\n",
    "        self.acc= tf.reduce_mean(tf.cast(self.correct_preds), tf.float32)  \n",
    "        return\n",
    "    \n",
    "    def entrenamiento(self, learning_rate =0.01):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= self.logits, labels= self.y_one_hot))\n",
    "        \n",
    "        optimizador = tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(loss)\n",
    "        \n",
    "        return  loss, optimizador\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Neural Network\n",
    "## This is a basic example of a Long Short Term Memory Neural Network with python using Tensorflow \n",
    "## I will be soon publishing a version with a training session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
