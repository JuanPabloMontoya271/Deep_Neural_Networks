{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier:\n",
    "    def __init__(self, seq_max_len,state_size, vocab_size, num_classes):\n",
    "        #init de los parametros con el constructor de la clase\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.state_size = state_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def build_model(self):\n",
    "        #el placeholder se utiliza para definir el proceso de entrada de las features\n",
    "        self.x = tf.placeholder(shape = [None, self.seq_max_len], dtype= tf.int32)\n",
    "        #se utiliza 'one hot encoding' para convertir valores categóricos a numéricos\n",
    "        x_one_hot = tf.one_hot(self.x, self.vocab_size)\n",
    "        x_one_hot = tf.cast(x_one_hot, tf.float32)\n",
    "        self.y = tf.placeholder(shape=[None], dtype= tf.int32)\n",
    "        self.y_one_hot = tf.one_hot(self.y, self.num_classes, dtype= tf.float32)\n",
    "        self.batch_size= tf.placeholder(tf.int32, [], name= 'batch_size')\n",
    "        #al ser un modelo sin capas ocultas, se considera lineal, por ende sigue la formula\n",
    "        # y = mx + b donde y es el resultado, m es la matriz de pesos y x es el vector de inputs\n",
    "        weights = {\n",
    "            \n",
    "            'layer_0': tf.Variable(tf.random_normal([self.seq_max_len*self.vocab_size, self.num_classes]))\n",
    "        }\n",
    "        biases = {\n",
    "            \n",
    "            'layer_0': tf.Variable(tf.random_normal([self.num_classes]))\n",
    "        }\n",
    "        \n",
    "        x_input =tf.reshape(x_one_hot, [-1, self.seq_max_len*self.vocab_size])\n",
    "        #se realiza la operación feedforward y=mx + b\n",
    "        output = tf.matmul(x_input, weights['layer_0']) + biases['layer_0']\n",
    "        self.logits = tf.sigmoid(output)\n",
    "        self.probs = tf.nn.softmax(self.logits, axis=1)\n",
    "        self.correct_preds = tf.equal(tf.argmax(self.probs,axis= 1),tf.argmax(self.y_one_hot, axis = 1))\n",
    "        self.acc= tf.reduce_mean(tf.cast(self.correct_preds), tf.float32())  \n",
    "        \n",
    "        \n",
    "    def entrenamiento(self, learning_rate =0.01):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= self.logits, labels= self.y_one_hot))\n",
    "        \n",
    "        optimizador = tf.train.AdamOptimizer(learning_rate= learning_rate).minimize(loss)\n",
    "        \n",
    "        return  loss, optimizador\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
